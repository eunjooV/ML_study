{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_path = '../data/.train/.task145/data/train.tsv'\n",
    "test_path = '../data/.train/.task145/data/test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>안경 써도 멋있네요 ㅎㅎ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1117공병단도..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>근데 국립묘지? 뭐냐? 이게?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>내가 지금 현역이였으면 웃고있을거같은데 ㅋㅋ 야외훈련 안하자나 그리고 밖에도 거의 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>저거 아직도 손으로 하고있네. 자주도하장비좀 도입해서 인력난 줄이고 기계로 좀 대체하자</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  tag\n",
       "0                                      안경 써도 멋있네요 ㅎㅎ    1\n",
       "1                                         1117공병단도..    0\n",
       "2                                   근데 국립묘지? 뭐냐? 이게?    2\n",
       "3  내가 지금 현역이였으면 웃고있을거같은데 ㅋㅋ 야외훈련 안하자나 그리고 밖에도 거의 ...    0\n",
       "4   저거 아직도 손으로 하고있네. 자주도하장비좀 도입해서 인력난 줄이고 기계로 좀 대체하자    2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>쪼수미 생각나</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K9대대에서 화학병으로 들어갔는데 잡부일만함</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이거 중국이 만들었다. 갑자기 생길일은 없잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>최고의 여전사입니다 충성!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>김민석 7사단인뎅 어디야 저기</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     comment tag\n",
       "0                    쪼수미 생각나   0\n",
       "1   K9대대에서 화학병으로 들어갔는데 잡부일만함   0\n",
       "2  이거 중국이 만들었다. 갑자기 생길일은 없잖아   0\n",
       "3            최고의 여전사입니다 충성!!   0\n",
       "4           김민석 7사단인뎅 어디야 저기   0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_documents(filename):\n",
    "    with open(filename, encoding=\"utf-8\") as f: # 윈도우는 꼭 encoding 해줘야 함\n",
    "        documents = [line.split('\\t') for line in f.read().splitlines()] \n",
    "        documents = documents[1:] # 첫번째 줄이 카테고리 이름적혀있는 줄이라서 날려버림\n",
    "        \n",
    "    return documents\n",
    "\n",
    "test = read_documents(test_path)\n",
    "test_df = pd.DataFrame(test)\n",
    "test_df.columns = [\"comment\", \"tag\"]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7100\n",
       "1    2847\n",
       "2    1373\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy \n",
    "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "def text_cleaning(doc):\n",
    "    # 한국어를 제외한 글자를 제거하는 함수.\n",
    "    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train data 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_df)):\n",
    "    text = text_cleaning(train_df['comment'][i])\n",
    "    train_df['comment'][i] = text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BoW(Bag of Words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_target = train_df['tag']\n",
    "X_features = train_df.drop('tag', axis=1, inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, \n",
    "                                                    test_size=0.2,\n",
    "                                                   random_state=156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 형태소 분석기 Mecab 사용\n",
    "- 텍스트에서 품사 정보를 부착하여 반환하는 pos로 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy \n",
    "from konlpy.tag import Mecab, Kkma, Okt, Komoran\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# 형태소 분석기 불러오기\n",
    "mecab = Mecab()\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        pos = ['{}/{}'.format(word,tag) for word, tag in pos]\n",
    "        return pos\n",
    "\n",
    "my_tokenizer = MyTokenizer(Mecab())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline을 이용하여 TfidfVectorizer와 Support Vector Machine을 결합하여 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 F1_score: 0.7981, 정밀도: 0.8373, 재현율: 0.7709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', TfidfVectorizer(tokenizer=my_tokenizer, ngram_range=(1,2), max_df=0.95)),\n",
    "    ('lr_clf', SVC(kernel='linear'))])\n",
    "\n",
    "pipeline.fit(X_train['comment'], y_train)\n",
    "pred = pipeline.predict(X_test['comment'])\n",
    "\n",
    "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, pred, average=\"macro\"),\n",
    "                                                precision_score(y_test, pred, average=\"macro\"), recall_score(y_test, pred, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV 이용하여 최적의 모델과 파라미터 값으로 예측 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'break_ties', 'cache_size', 'class_weight', 'coef0', 'decision_function_shape', 'degree', 'gamma', 'kernel', 'max_iter', 'probability', 'random_state', 'shrinking', 'tol', 'verbose'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 11.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'} 0.7492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "SVC_linear = SVC(random_state=0)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=my_tokenizer, ngram_range=(1,2), max_df=0.95)\n",
    "tfidf_vect.fit(X_train['comment'])\n",
    "tfidf_matrix_train = tfidf_vect.transform(X_train['comment'])\n",
    "\n",
    "# 하이퍼라미터 C, gamma, kernel의 최적화를 위해 GridSearchCV이용\n",
    "param_range = [0.01, 0.1, 1, 10]\n",
    "params = [{'C':param_range,\n",
    "          'gamma':param_range,\n",
    "          'kernel':['rbf']},\n",
    "          \n",
    "        {'C':param_range,\n",
    "         'kernel':['linear']}]\n",
    "\n",
    "grid_cv = GridSearchCV(SVC_linear, param_grid=params, cv=3, scoring='f1_macro', verbose=1)\n",
    "grid_cv.fit(tfidf_matrix_train, y_train)\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 F1_score: 0.8045, 정밀도: 0.8266, 재현율: 0.7868\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix_test = tfidf_vect.transform(X_test['comment'])\n",
    "\n",
    "# classifier 는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "\n",
    "print('예측 F1_score: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}'.format(f1_score(y_test, preds, average=\"macro\"),\n",
    "                                                precision_score(y_test, preds, average=\"macro\"), recall_score(y_test, preds, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_estimator, './model/model_145.pkl') \n",
    "\n",
    "grid_model = joblib.load('./model/model_145.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data 전처리 & 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 2,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(test_df)):\n",
    "    text = text_cleaning(test_df['comment'][i])\n",
    "    test_df['comment'][i] = text\n",
    "    \n",
    "    \n",
    "test_final = tfidf_vect.transform(test_df['comment'])\n",
    "\n",
    "preds_test = grid_model.predict(test_final)\n",
    "preds_test[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tag']= preds_test\n",
    "submission = test_df['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./prediction/prediction_145.csv', index=False, header=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './model_145.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b574873f61b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 압축파일에 모델 파일 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/model_145.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_145.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, filename, arcname, compress_type, compresslevel)\u001b[0m\n\u001b[1;32m   1739\u001b[0m             )\n\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1741\u001b[0;31m         zinfo = ZipInfo.from_file(filename, arcname,\n\u001b[0m\u001b[1;32m   1742\u001b[0m                                   strict_timestamps=self._strict_timestamps)\n\u001b[1;32m   1743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/zipfile.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, filename, arcname, strict_timestamps)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m         \u001b[0misdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mmtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model_145.pkl'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('./model/model_145.zip', mode='w') as f:\n",
    "    f.write('./육군_최종.ipynb', compress_type=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "# 압축파일에 모델 파일 추가\n",
    "with zipfile.ZipFile('./model/model_145.zip', mode='a') as f:\n",
    "    f.write('./model/model_145.pkl', compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nipa.taskSubmit import nipa_submit\n",
    "\n",
    "team_id=\"1345\"\n",
    "task_no=\"145\"\n",
    "result_path = './prediction/prediction_145.csv'\n",
    "model_path = './model/model_145.pkl'\n",
    "nipa_submit(team_id=team_id,\n",
    "            task_no=task_no,\n",
    "            result=result_path,\n",
    "            model=model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
